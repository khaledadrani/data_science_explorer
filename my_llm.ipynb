{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8efcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/khaled-\n",
      "[nltk_data]     adrani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07394ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./venv/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./venv/lib/python3.10/site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a7a243",
   "metadata": {},
   "source": [
    "# WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "037bfb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence 1: ['I', 'love', 'programming', '.']\n",
      "Tokenized sentence 2: ['NLTK', 'is', 'a', 'great', 'toolkit', 'for', 'natural', 'language', 'processing', '.']\n",
      "Tokenized sentence 3: ['Tokenization', 'is', 'an', 'important', 'step', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentences = [\n",
    "    \"I love programming.\",\n",
    "    \"NLTK is a great toolkit for natural language processing.\",\n",
    "    \"Tokenization is an important step in NLP.\"\n",
    "]\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Print the tokenized sentences\n",
    "for i, sentence in enumerate(tokenized_sentences, 1):\n",
    "    print(f\"Tokenized sentence {i}: {sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99299af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\software\\data-science-explorer\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)/main/tokenizer.json: 100%|█████████████████████████████████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 810kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs for sentence 1: [101, 1045, 2293, 4730, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Token IDs for sentence 2: [101, 17953, 2102, 2243, 2003, 1037, 2307, 6994, 23615, 2005, 3019, 2653, 6364, 1012, 102]\n",
      "Token IDs for sentence 3: [101, 19204, 3989, 2003, 2019, 2590, 3357, 1999, 17953, 2361, 1012, 102, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "sentences = [\n",
    "    \"I love programming.\",\n",
    "    \"NLTK is a great toolkit for natural language processing.\",\n",
    "    \"Tokenization is an important step in NLP.\"\n",
    "]\n",
    "\n",
    "# Load a pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the sentences and obtain token IDs\n",
    "tokenized_inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get the token IDs as a list\n",
    "token_ids = tokenized_inputs[\"input_ids\"].tolist()\n",
    "\n",
    "# Print the token IDs\n",
    "for i, ids in enumerate(token_ids, 1):\n",
    "    print(f\"Token IDs for sentence {i}: {ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01f65f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] i love programming. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] nltk is a great toolkit for natural language processing. [SEP]\n",
      "[CLS] tokenization is an important step in nlp. [SEP] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# Print the decoded tokens\n",
    "for i, ids in enumerate(token_ids):\n",
    "    # Convert token IDs to string\n",
    "    decoded_tokens = tokenizer.decode(ids)\n",
    "    print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eef64b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.325103  , -1.0814596 , -0.19151801, -1.0917969 ,  0.3774717 ,\n",
       "        2.1193674 ,  0.14831299,  1.1750474 ,  0.27608478,  3.5640159 ,\n",
       "       -0.21550748, -0.3872805 , -0.01346767, -1.378101  ,  1.5121084 ,\n",
       "       -0.08623586, -1.933989  , -0.11408056, -0.9860206 , -2.4706352 ,\n",
       "       -1.1276447 , -0.4152274 ,  2.1197853 ,  1.7633233 , -0.2526904 ,\n",
       "       -0.7457644 ,  1.0953835 ,  0.11857101,  0.61220956, -1.3049716 ,\n",
       "        1.3378174 , -0.29886734, -0.29212442, -0.4469438 ,  0.35449713,\n",
       "       -2.0813339 , -2.4365878 , -1.4595014 , -0.13775744, -2.192705  ,\n",
       "        1.5707886 , -0.27742532,  0.73836327,  1.3525312 , -0.7743389 ,\n",
       "       -0.77003706, -0.02322291,  0.77104056,  0.31869486, -0.13776901,\n",
       "        0.19484688, -1.2495929 ,  0.498856  , -0.00436315, -0.16236602,\n",
       "       -0.6067438 ,  0.8008451 ,  2.8110933 ,  0.69573265, -0.7279365 ,\n",
       "       -0.3188081 , -1.1669003 ,  1.6769878 , -1.578182  , -0.14196508,\n",
       "       -1.8940164 ,  1.4671096 ,  1.7788589 , -0.8679901 , -0.15584452,\n",
       "       -0.13961807, -0.09730375,  0.49901256, -0.45330203,  0.8772218 ,\n",
       "       -1.2259632 , -0.03491092,  2.9636195 , -1.7653301 , -0.90529907,\n",
       "        0.5244117 ,  0.16144626, -2.2888186 , -0.6361609 ,  0.7979543 ,\n",
       "       -0.9861773 ,  1.9256904 ,  0.7562646 ,  0.38272035,  2.296901  ,\n",
       "        0.37406802,  0.3630618 ,  0.5928567 , -0.8194863 ,  0.130174  ,\n",
       "        1.5654953 ,  0.9809555 ,  0.63848054,  1.4237264 , -1.2110319 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.wv['japan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "078afaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Words to 'country': [('region', 0.7159584164619446), ('land', 0.6805279850959778), ('nation', 0.658952534198761), ('sector', 0.6567452549934387), ('measure', 0.6284477114677429), ('village', 0.6263555884361267), ('garden', 0.6238498091697693), ('town', 0.6204319596290588), ('wilderness', 0.6184377074241638), ('economy', 0.6140435338020325)]\n"
     ]
    }
   ],
   "source": [
    "# Get word vector for a specific word\n",
    "def compare_words(word, model):\n",
    "    word_vector = model.wv[word]\n",
    "\n",
    "    # Find similar words\n",
    "    similar_words = model.wv.most_similar(word)\n",
    "\n",
    "    # Print results\n",
    "#     print(f\"Word Vector for '{word}': {word_vector}\")\n",
    "    print(f\"Similar Words to '{word}': {similar_words}\")\n",
    "    \n",
    "    return similar_words\n",
    "    \n",
    "word = \"country\"\n",
    "_ = compare_words(word, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c85caac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Key 'lately?' not present\"\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the state of Japan lately?\"\n",
    "\n",
    "new_sentence = query\n",
    "# Split the sentence into words\n",
    "words = new_sentence.lower().split()\n",
    "\n",
    "oov_vector = [0.0] * model.vector_size  # Assign a random vector or all zeros\n",
    "\n",
    "\n",
    "# Compute the average embedding\n",
    "sentence_embedding_sum = []\n",
    "\n",
    "for word in words:\n",
    "    try:\n",
    "        vec = model.wv[word]\n",
    "    except KeyError as error:\n",
    "        print(error)\n",
    "        vec = oov_vector\n",
    "    sentence_embedding_sum.append(vec)\n",
    "        \n",
    "    \n",
    "    \n",
    "sentence_embedding = sum(sentence_embedding_sum) / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9cbbb9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.6684510707855225\n",
      "concern 0.6658481359481812\n",
      "policy 0.6452551484107971\n",
      "measure 0.6338269114494324\n",
      "view 0.6305679678916931\n",
      "country 0.6284566521644592\n",
      "world 0.6249071955680847\n",
      "situation 0.6224737167358398\n",
      "problems 0.620403528213501\n",
      "economy 0.6164305806159973\n"
     ]
    }
   ],
   "source": [
    "# Find the most similar words\n",
    "similar_words = model.wv.most_similar(positive=[sentence_embedding], topn=10)\n",
    "\n",
    "# Print the most similar words\n",
    "for word, similarity in similar_words:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97f407e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Vector for 'I': [-4.88621801e-01 -1.40371811e+00 -1.39933318e-01  1.06850706e-01\n",
      "  1.02041435e+00 -2.22615689e-01  2.75205951e-02  1.12041712e+00\n",
      " -1.21569169e+00  1.12856403e-01 -8.95855650e-02  8.30142871e-02\n",
      " -6.56571686e-01  1.30964851e+00  3.64047140e-01  5.38011491e-01\n",
      " -5.37430048e-01 -1.51941374e-01  4.55765158e-01  3.98258448e-01\n",
      " -1.38316467e-01 -4.49481159e-01 -1.51659131e+00 -1.16060197e+00\n",
      "  4.38162953e-01  1.36776134e-01 -8.98952782e-01  3.42559814e-01\n",
      " -2.75909845e-02 -6.08607709e-01 -1.12204149e-01  4.31124866e-01\n",
      "  4.22272563e-01  3.68530124e-01 -2.09876690e-02 -1.09747119e-01\n",
      " -1.22736312e-01  1.95901453e-01 -2.79443003e-02  8.52847695e-01\n",
      " -2.90774971e-01 -3.17778558e-01  9.51572239e-01  1.95836142e-01\n",
      " -1.57318980e-01 -5.18942475e-01  4.68272626e-01 -5.29799104e-01\n",
      "  4.45801914e-01 -3.91309671e-02  3.04106951e-01 -3.67694885e-01\n",
      "  2.44503692e-01  6.94566607e-01 -2.64469862e-01  2.73315817e-01\n",
      "  3.21534514e-01 -1.09112464e-01  3.19945246e-01 -5.23173511e-01\n",
      "  1.46419764e-01 -1.48782870e-02  7.48982787e-01  2.35898018e-01\n",
      " -1.78792453e+00  1.02411890e+00  8.47361386e-01 -2.40542763e-03\n",
      "  7.93799520e-01  1.68916142e+00 -4.51332897e-01 -2.24225700e-01\n",
      "  5.35463020e-02 -1.44621148e-03  7.98828602e-01  7.86827445e-01\n",
      "  4.34700429e-01  1.54868081e-01 -3.42725873e-01 -1.92696422e-01\n",
      " -1.90006882e-01 -1.07769752e-02 -8.26064289e-01  3.10222358e-01\n",
      " -2.57125050e-01  5.56767583e-01 -8.59422922e-01 -4.07058448e-01\n",
      " -1.70669138e-01 -9.14901257e-01  6.96151793e-01  8.87783229e-01\n",
      " -1.30843788e-01  4.37764913e-01 -2.14055017e-01  8.04235280e-01\n",
      "  9.14338648e-01 -6.14193499e-01 -3.08949679e-01  2.72023022e-01]\n",
      "Similar Words to 'I': [('Y', 0.4218229055404663), ('y', 0.252797394990921), ('B', 0.21075616776943207), (']', 0.2087993025779724), ('\"', 0.18925084173679352), ('/', 0.17815521359443665), ('W', 0.16929380595684052), ('D', 0.16154971718788147), ('*', 0.14543616771697998), (\"'\", 0.12986239790916443)]\n"
     ]
    }
   ],
   "source": [
    "compare_words(word, model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd3e1ad",
   "metadata": {},
   "source": [
    "# NGRAM models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbe73e2",
   "metadata": {},
   "source": [
    "## 2gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5812963",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(generated_text)\n\u001b[0;32m---> 30\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m <end> \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43msentences\u001b[49m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(corpus))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from nltk import ngrams\n",
    "\n",
    "def generate_2gram_model(text):\n",
    "    model = {}\n",
    "    ngram_pairs = ngrams(text.split(), 2)\n",
    "    \n",
    "    for word1, word2 in ngram_pairs:\n",
    "        if word1 in model:\n",
    "            model[word1].append(word2)\n",
    "        else:\n",
    "            model[word1] = [word2]\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_text(model, seed, length):\n",
    "    current_word = seed\n",
    "    generated_text = [current_word]\n",
    "    \n",
    "    for _ in range(length - 1):\n",
    "        if current_word in model:\n",
    "            next_word = random.choice(model[current_word])\n",
    "            generated_text.append(next_word)\n",
    "            current_word = next_word\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return ' '.join(generated_text)\n",
    "\n",
    "corpus = \" <end> \".join(sentences)\n",
    "print(len(corpus))\n",
    "# Example usage\n",
    "\n",
    "text = corpus\n",
    "model = generate_2gram_model(text)\n",
    "generated_text = generate_text(model, seed=\"Hello\", length=10)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "05f3ec98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(model, seed=\"Hello\", length=10)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbd72fe",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6694f13",
   "metadata": {},
   "source": [
    "## gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb2858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab97b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ed18b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "65d550d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70f38aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def get_sample(corpus_object):\n",
    "    file_ids = corpus_object.fileids()\n",
    "    return [corpus_object.sents(file_id) for file_id in file_ids]\n",
    "\n",
    "def transform_into_sentences(corpus_object):\n",
    "    sentences = []\n",
    "    file_ids = corpus_object.fileids()\n",
    "\n",
    "    # Extract sentences from each file in the Gutenberg Corpus\n",
    "    for file_id in file_ids:\n",
    "        file_sentences = corpus_object.sents(file_id)\n",
    "        for sentence in file_sentences:\n",
    "            # Convert the sentence list to a string\n",
    "            sentence_str = ' '.join(sentence)\n",
    "            sentences.append(sentence_str)\n",
    "    return sentences\n",
    "    \n",
    "def get_data_from_nltk():\n",
    "    corpus_list = [\n",
    "        'pros_cons','reuters','sentiwordnet','gutenberg','senseval',\n",
    "        'sinica_treebank','comparative_sentences',\n",
    "        'twitter'\n",
    "    ]\n",
    "\n",
    "    for item in corpus_list:\n",
    "        try:\n",
    "            nltk.download(item)\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            \n",
    "    from nltk.corpus import pros_cons\n",
    "    from nltk.corpus import sentiwordnet, pros_cons,reuters,sentiwordnet,gutenberg,senseval, sinica_treebank,comparative_sentences\n",
    "    from nltk.corpus import TwitterCorpusReader\n",
    "    corpus_objects_ls = [ pros_cons,reuters ,gutenberg, sinica_treebank,comparative_sentences\n",
    "    ]\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "                \n",
    "    nltk_total_corpus = []\n",
    "    for c in corpus_objects_ls:\n",
    "        nltk_total_corpus.extend(transform_into_sentences(c))\n",
    "        \n",
    "    print(nltk_total_corpus[100:105])\n",
    "    \n",
    "    return nltk_total_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3840ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import pros_cons\n",
    "from nltk.corpus import sentiwordnet, pros_cons,reuters,sentiwordnet,gutenberg,senseval, sinica_treebank,comparative_sentences\n",
    "from nltk.corpus import TwitterCorpusReader\n",
    "corpus_objects_ls = [ pros_cons,reuters ,gutenberg, sinica_treebank,comparative_sentences\n",
    "]\n",
    "\n",
    "\n",
    "def get_sample(corpus_object):\n",
    "    file_ids = corpus_object.fileids()\n",
    "    return [corpus_object.sents(file_id) for file_id in file_ids]\n",
    "\n",
    "def transform_into_sentences(corpus_object):\n",
    "    sentences = []\n",
    "    file_ids = corpus_object.fileids()\n",
    "\n",
    "    # Extract sentences from each file in the Gutenberg Corpus\n",
    "    for file_id in file_ids:\n",
    "        file_sentences = corpus_object.sents(file_id)\n",
    "        for sentence in file_sentences:\n",
    "            # Convert the sentence list to a string\n",
    "            sentence_str = ' '.join(sentence)\n",
    "            sentences.append(sentence_str)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98e730fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package pros_cons to /home/khaled-\n",
      "[nltk_data]     adrani/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data] Downloading package reuters to /home/khaled-\n",
      "[nltk_data]     adrani/nltk_data...\n",
      "[nltk_data] Downloading package sentiwordnet to /home/khaled-\n",
      "[nltk_data]     adrani/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data] Downloading package gutenberg to /home/khaled-\n",
      "[nltk_data]     adrani/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package senseval to /home/khaled-\n",
      "[nltk_data]     adrani/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/senseval.zip.\n",
      "[nltk_data] Downloading package sinica_treebank to /home/khaled-\n",
      "[nltk_data]     adrani/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data] Downloading package comparative_sentences to /home/khaled-\n",
      "[nltk_data]     adrani/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data] Error loading twitter: Package 'twitter' not found in\n",
      "[nltk_data]     index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['printing photos is hard for me', 'eats batteries quickly , viewfinder too far from lens , sporadic LCD window', \"As with all digital cam ' s ... eats batteries .\", 'Batteries can go fast if you dont get good ones', 'Has flaws which are made worse by bad service policy .']\n"
     ]
    }
   ],
   "source": [
    "            \n",
    "nltk_total_corpus = get_data_from_nltk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afbb1f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['printing photos is hard for me',\n",
       " 'eats batteries quickly , viewfinder too far from lens , sporadic LCD window',\n",
       " \"As with all digital cam ' s ... eats batteries .\",\n",
       " 'Batteries can go fast if you dont get good ones',\n",
       " 'Has flaws which are made worse by bad service policy .']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_total_corpus[100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2982fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b0eb84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5dcae26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 36718\n",
      "1 1801350\n",
      "2 error!\n",
      "2 25000\n",
      "3 error!\n",
      "3 120000\n",
      "4 error!\n",
      "4 560000\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import WikiText2, WikiText103, YelpReviewPolarity, AG_NEWS, IMDB\n",
    "\n",
    "datasets = [WikiText2, WikiText103, IMDB, AG_NEWS, YelpReviewPolarity]\n",
    "\n",
    "# Load train, val, and test splits from each dataset\n",
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "for index, dataset in enumerate(datasets):\n",
    "    try:\n",
    "        train_dataset, val_dataset, test_dataset = dataset()\n",
    "        train_dataset = list(train_dataset)\n",
    "        print(index, len(train_dataset))\n",
    "        train_data.extend(train_dataset)\n",
    "        val_data.extend(val_dataset)\n",
    "        test_data.extend(test_dataset)\n",
    "    except:\n",
    "        print(index, 'error!')\n",
    "        train_dataset,  test_dataset = dataset()\n",
    "        train_dataset,  test_dataset = list(train_dataset),  list(test_dataset)\n",
    "        print(index, len(train_dataset))\n",
    "        train_data.extend([e for e in train_dataset])\n",
    "        test_data.extend([e for e in test_dataset])\n",
    "\n",
    "combined_data = train_data + val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d0b0cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_token=\"<end>\\n\" \n",
    "def get_corpus(docs, end_token=\"<end>\\n\"):\n",
    "    return end_token.join(str(d).lower() for d in docs)\n",
    "\n",
    "large_corpus = get_corpus(combined_data + nltk_total_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf52f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json \n",
    "\n",
    "with open('large_corpus_nltk_pytorch.json','w') as f:\n",
    "    json.dump({\"data\":large_corpus},f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9618f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import json\n",
    "    \n",
    "with open('large_corpus_nltk_pytorch.json','rb') as f:\n",
    "    raw_corpus = json.load(f)['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71ecba98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1067297403"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f43441c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222894, [[], ['=', 'valkyria', 'chronicles', 'iii', '=']])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = raw_corpus[:67297403].lower().split(\"<end>\\n\")\n",
    "to_vectorize = [e.split() for e in tokenized if e]\n",
    "len(tokenized), to_vectorize[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d15ff90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "embed = Word2Vec(sentences=to_vectorize, vector_size=500, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cbb0d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"demon\" in embed.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5abc059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_embedding(word, embed, verbose=False):\n",
    "    try:\n",
    "        return embed.wv[word]\n",
    "    except KeyError as error:\n",
    "        if verbose:\n",
    "            print(error)\n",
    "        return np.array([0.0] * embed.vector_size,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bc59028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Words to 'demon': [('ravana', 0.7389745712280273), ('spear', 0.6977938413619995), ('harsha', 0.6823989748954773), ('mysterious', 0.6727192997932434), ('indu', 0.6691309213638306), ('scar', 0.6663864850997925), ('rama', 0.6658058166503906), ('raghuveer', 0.6629301905632019), ('bubblegum', 0.6627861857414246), ('lion', 0.660142183303833)]\n"
     ]
    }
   ],
   "source": [
    "# Get word vector for a specific word\n",
    "def compare_words(word, model):\n",
    "    word_vector = get_embedding(word,model)\n",
    "\n",
    "    # Find similar words\n",
    "    similar_words = model.wv.most_similar(word)\n",
    "\n",
    "    # Print results\n",
    "#     print(f\"Word Vector for '{word}': {word_vector}\")\n",
    "    print(f\"Similar Words to '{word}': {similar_words}\")\n",
    "    \n",
    "    return similar_words\n",
    "\n",
    "cmp = compare_words('demon', embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0165c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_weight(vectors):\n",
    "    return sum(vector for vector in vectors) / len(vectors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55b9b058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.73904872e-01, -2.83734709e-01, -2.68028915e-01, -3.70237350e-01,\n",
       "        2.26001635e-01, -1.38239264e-01,  3.65052134e-01, -4.28179428e-02,\n",
       "       -3.03795815e-01,  4.66121465e-01, -2.08851963e-01,  3.95463437e-01,\n",
       "       -3.08676094e-01, -2.82006919e-01,  6.44253373e-01,  2.79514343e-01,\n",
       "        3.39981079e-01,  4.83988738e-03,  1.47219673e-01,  4.10906643e-01,\n",
       "       -5.95335126e-01, -1.94823086e-01, -2.75833547e-01,  3.96358192e-01,\n",
       "       -5.59105337e-01, -8.74587148e-03,  2.19336465e-01, -6.12855613e-01,\n",
       "        3.59320432e-01,  1.89020172e-01,  1.32271573e-01, -3.43840599e-01,\n",
       "        1.11938633e-01,  1.53857931e-01,  2.07087770e-01,  5.23193121e-01,\n",
       "       -4.67054754e-01,  2.95638353e-01,  1.75057516e-01,  3.08911532e-01,\n",
       "       -4.33479100e-01,  6.88569108e-03,  8.61960799e-02, -5.37465572e-01,\n",
       "       -2.99694419e-01,  9.70285892e-01,  6.22661173e-01,  5.30795008e-02,\n",
       "       -9.29383576e-01,  6.03600979e-01, -3.86647284e-01,  2.40300857e-02,\n",
       "       -3.20376039e-01,  1.14439219e-01,  1.41620651e-01, -1.60934761e-01,\n",
       "        4.74261306e-02, -8.14563215e-01, -4.54235703e-01,  4.03259367e-01,\n",
       "       -2.92087272e-02, -5.82110703e-01, -5.16466975e-01, -7.63004348e-02,\n",
       "       -6.17719293e-01, -5.18605828e-01,  1.90524250e-01, -3.88704181e-01,\n",
       "        7.17809051e-02,  5.71756363e-01,  9.19863820e-01,  1.30289216e-02,\n",
       "        1.38188958e-01, -3.13533366e-01, -2.66996510e-02,  1.47703543e-01,\n",
       "       -8.93444642e-02, -2.07890302e-01,  3.51343974e-02,  2.58414187e-02,\n",
       "       -3.40552360e-01, -2.51872152e-01,  6.38394713e-01, -3.14726651e-01,\n",
       "       -3.88179570e-01,  3.44039679e-01, -2.77274668e-01,  4.19879049e-01,\n",
       "        1.25920594e+00, -2.34377354e-01,  6.37939394e-01,  3.29080373e-01,\n",
       "        3.76479626e-01,  6.85796022e-01, -1.64385647e-01,  1.25652894e-01,\n",
       "        2.59297341e-01, -3.21494162e-01, -5.64127378e-02,  1.82819396e-01,\n",
       "       -9.92954597e-02,  5.58012009e-01,  2.27229558e-02,  2.12532375e-02,\n",
       "       -2.16505374e-03,  5.32137632e-01,  4.48659584e-02, -2.73964167e-01,\n",
       "       -1.06600963e-01,  3.70222330e-01,  5.93907416e-01, -1.26511782e-01,\n",
       "       -4.65895295e-01,  2.31658772e-01, -7.28284195e-02, -2.88945973e-01,\n",
       "       -7.11349845e-01, -8.40624094e-01,  6.12742901e-01, -3.20480525e-01,\n",
       "        4.06322986e-01,  6.47813380e-01,  3.26044440e-01, -4.21748087e-02,\n",
       "       -2.84441859e-01, -5.42928651e-03,  4.14844960e-01, -3.90387237e-01,\n",
       "       -1.49331927e-01,  2.10777164e-01, -2.95574188e-01, -3.81825954e-01,\n",
       "        6.44988540e-06,  5.54561198e-01,  3.50629538e-01, -8.23260099e-02,\n",
       "       -2.45657772e-01,  3.14637601e-01,  3.26496094e-01, -1.70295015e-01,\n",
       "       -4.33721185e-01,  7.72427678e-01,  3.16519797e-01, -1.75950304e-01,\n",
       "        7.68847689e-02, -2.26999506e-01,  3.92749786e-01,  4.70103323e-01,\n",
       "        1.45099955e-02,  3.41660857e-01,  1.31442145e-01, -1.08392537e-01,\n",
       "       -3.59136671e-01,  2.76938170e-01,  2.41206869e-01, -1.59887597e-01,\n",
       "        1.00629771e+00,  6.75885260e-01,  2.68815998e-02,  4.53145169e-02,\n",
       "       -1.96486801e-01, -2.03197062e-01, -2.75236279e-01, -2.75616109e-01,\n",
       "       -3.23624521e-01,  5.50775409e-01, -9.76436958e-02, -1.28576487e-01,\n",
       "        2.67500848e-01, -1.57518849e-01,  1.01315796e-01, -2.22776935e-01,\n",
       "        4.95049357e-01,  1.66224465e-02,  3.01121742e-01,  7.16117173e-02,\n",
       "       -1.85382903e-01,  5.54652989e-01,  1.59794822e-01,  2.43351653e-01,\n",
       "       -1.58834964e-01, -1.07523248e-01, -2.90757567e-01,  2.37368733e-01,\n",
       "        6.15497053e-01,  4.16855514e-01,  3.59451085e-01,  3.99003893e-01,\n",
       "        5.29160678e-01,  2.21008256e-01,  3.51206101e-02, -1.02333045e+00,\n",
       "       -2.58922935e-01,  7.82509983e-01,  3.93620253e-01, -4.04270023e-01,\n",
       "        1.17239676e-01, -2.99430311e-01,  3.83526891e-01, -2.61095524e-01,\n",
       "       -2.65219748e-01, -2.07359686e-01, -2.81837106e-01, -6.19561747e-02,\n",
       "       -9.00996476e-02,  8.27830508e-02,  9.62597206e-02, -3.96920517e-02,\n",
       "        1.92767948e-01,  5.52866459e-01, -5.07493675e-01, -3.98807764e-01,\n",
       "        2.49427333e-01, -3.89144421e-01, -7.28139222e-01,  6.11180544e-01,\n",
       "        3.33662778e-01, -1.92196425e-02,  3.09234232e-01,  5.13771772e-01,\n",
       "       -1.05206810e-01,  4.99006629e-01,  4.62474003e-02,  6.74154401e-01,\n",
       "       -2.72392809e-01,  2.00526305e-02,  1.52874157e-01,  6.70302868e-01,\n",
       "       -6.41358137e-01, -5.25934160e-01,  1.69932038e-01, -2.39932612e-02,\n",
       "        1.81581154e-01, -2.46865764e-01, -9.78840292e-02, -1.81572959e-01,\n",
       "       -1.72675610e-01,  2.87387311e-01, -3.84071842e-02,  5.30659676e-01,\n",
       "       -1.64867133e-01, -2.62549520e-01,  1.88897416e-01, -3.30098420e-01,\n",
       "        2.31810883e-02,  2.84549922e-01,  6.74707234e-01, -1.26817627e-02,\n",
       "        2.62203693e-01, -1.00715965e-01, -2.64885277e-01, -2.69646645e-01,\n",
       "       -5.54112613e-01, -1.61643237e-01,  5.73986769e-01, -4.42211986e-01,\n",
       "        8.97019543e-03,  3.23354781e-01,  5.90480790e-02,  2.86438853e-01,\n",
       "       -4.40388054e-01,  5.88226356e-02,  5.84243573e-02,  2.41645858e-01,\n",
       "       -8.30007568e-02,  1.35733068e-01, -2.61956304e-01, -1.43632321e-02,\n",
       "        4.16003525e-01,  6.11300886e-01,  2.89083183e-01, -3.27244066e-02,\n",
       "       -5.46601474e-01,  1.87492535e-01,  5.91813862e-01, -4.44426566e-01,\n",
       "        3.76773566e-01, -4.05233562e-01,  3.41933817e-01,  1.62590779e-02,\n",
       "       -6.50541186e-01,  3.58936280e-01, -1.44013643e-01,  3.13813724e-02,\n",
       "       -1.95741886e-03, -8.83536339e-02,  6.56991959e-01,  3.66838932e-01,\n",
       "        5.52164257e-01,  2.71103978e-01,  9.65474024e-02,  1.12870169e+00,\n",
       "        4.58501488e-01,  1.41439587e-01,  2.25543231e-01,  2.08935320e-01,\n",
       "        1.08705319e-01,  1.11102924e-01,  3.64558458e-01,  4.88809794e-01,\n",
       "       -3.09193432e-01,  9.70908999e-01,  4.79184836e-01, -1.03608035e-01,\n",
       "        5.44402957e-01, -1.90031260e-01,  3.96978915e-01, -1.25225455e-01,\n",
       "       -2.91799664e-01, -1.79522261e-01, -5.07921457e-01, -3.60245407e-01,\n",
       "        3.19678277e-01, -6.22376539e-02, -1.41428232e-01,  2.84714047e-02,\n",
       "       -1.87877283e-01,  1.80979222e-01,  3.65035594e-01, -9.64798748e-01,\n",
       "        3.62241119e-01,  4.70746517e-01, -5.61140120e-01,  2.31910512e-01,\n",
       "        7.24054456e-01, -8.76365542e-01,  1.64385498e-01, -1.95187584e-01,\n",
       "       -2.43194804e-01, -4.68959540e-01,  6.32323682e-01,  4.08118486e-01,\n",
       "        5.58636010e-01, -1.11541450e-01,  2.81423688e-01,  1.76062629e-01,\n",
       "        4.20548409e-01,  7.48722374e-01,  2.40548402e-01, -4.02583152e-01,\n",
       "        2.65815288e-01, -2.07709014e-01, -3.95685554e-01, -7.61180744e-02,\n",
       "        1.89070404e-01,  5.34854770e-01, -4.84975666e-01, -2.68574446e-01,\n",
       "       -6.96187094e-02,  2.04740897e-01, -1.01607680e-01, -4.00708824e-01,\n",
       "       -3.03274632e-01,  5.96520066e-01,  3.51970732e-01,  2.08793595e-01,\n",
       "       -1.82770953e-01, -7.49861419e-01, -3.61961544e-01, -9.55151543e-02,\n",
       "        3.03043038e-01,  6.26602113e-01,  1.98690534e-01,  3.01749378e-01,\n",
       "        6.15298927e-01, -6.10747099e-01,  3.01445186e-01, -8.99656042e-02,\n",
       "       -3.92806202e-01,  6.56741560e-01, -7.73106992e-01, -8.32163841e-02,\n",
       "       -4.98996019e-01, -7.55016208e-01,  1.89262196e-01,  1.84603315e-02,\n",
       "       -4.89078462e-01, -7.32696831e-01, -5.14093459e-01, -4.26182896e-01,\n",
       "        2.80477405e-01, -2.57684022e-01,  6.45348802e-02,  9.02729034e-01,\n",
       "       -3.53393137e-01, -4.53068614e-01, -2.36400291e-01, -4.43440139e-01,\n",
       "       -4.97946024e-01,  5.20418406e-01,  1.03812560e-01, -3.58224034e-01,\n",
       "        9.74544659e-02,  2.22182438e-01, -2.97564238e-01,  1.91219166e-01,\n",
       "        1.65887967e-01, -8.50498304e-02,  1.07929632e-01,  3.31193328e-01,\n",
       "       -4.34102342e-02,  2.59464592e-01, -1.38905436e-01, -6.59017146e-01,\n",
       "        8.41550976e-02, -1.02879107e+00, -3.51686239e-01,  8.93805325e-01,\n",
       "        3.42019051e-01, -2.97262613e-02,  1.74910516e-01, -1.79976612e-01,\n",
       "        7.17327371e-02, -2.49345243e-01, -1.61233246e-01, -2.92295292e-02,\n",
       "        1.67335924e-02, -3.86997163e-01, -1.21512897e-01, -2.73036689e-01,\n",
       "        8.28702450e-01,  1.87209144e-01, -2.27270707e-01, -9.64461565e-02,\n",
       "       -3.95721197e-01, -3.43506753e-01, -4.86910164e-01, -5.65007981e-03,\n",
       "        7.20633790e-02,  1.29783049e-01,  9.31300372e-02, -4.68308199e-03,\n",
       "       -6.34749115e-01, -5.87244108e-02,  2.11900204e-01, -3.27271461e-01,\n",
       "        3.91389519e-01, -1.46960497e-01, -4.16292340e-01,  7.27701187e-01,\n",
       "       -5.00578701e-01, -2.78915942e-01, -1.06173847e-02, -7.88906813e-01,\n",
       "       -3.72823685e-01,  2.66015738e-01, -2.69888341e-01,  2.13781863e-01,\n",
       "       -1.68242931e-01,  7.23405695e-03,  5.28458171e-02,  7.37971246e-01,\n",
       "       -1.49573863e-01,  3.21392566e-01, -1.84038490e-01, -7.16760278e-01,\n",
       "       -2.39548951e-01,  1.13989571e-02,  2.29774356e-01,  5.39749265e-01,\n",
       "       -1.00822791e-01, -8.87245536e-02, -6.23704016e-01, -1.58986971e-01,\n",
       "       -8.91955793e-02,  2.91988049e-02, -1.95107102e-01, -5.89413702e-01,\n",
       "       -8.02321076e-01,  3.41229364e-02, -4.98889536e-01, -1.73165143e-01,\n",
       "        1.48037346e-02,  3.72726947e-01, -6.15441859e-01, -4.44074392e-01,\n",
       "        4.19915676e-01,  2.31333897e-01, -4.18523774e-02, -3.54937813e-03,\n",
       "       -6.67950153e-01, -8.46527815e-01,  8.97502527e-02,  2.83102512e-01,\n",
       "        3.37193072e-01, -3.80722046e-01, -9.35280696e-02,  4.51865792e-01,\n",
       "        8.32543150e-02, -5.11104524e-01,  2.19480708e-01, -8.76547396e-01,\n",
       "        5.59682250e-01,  3.19125473e-01, -1.28595740e-01,  1.02485858e-01,\n",
       "        5.70158422e-01, -5.17440975e-01, -3.38855118e-01,  2.01190099e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Union, List\n",
    "def compute_avg_weight_from_string(query:Union[str,List[str]], embed):\n",
    "    if isinstance(query, str):\n",
    "        vectors = [get_embedding(word, embed) for word in query.split()]\n",
    "    else:\n",
    "        vectors = [get_embedding(word, embed) for word in query]\n",
    "    return compute_avg_weight(vectors)\n",
    "compute_avg_weight_from_string([\"I\",\"love\"], embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc6a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad5b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a6504b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.88877094, -0.7881048 , -0.8148203 , -0.31768516,  0.30527776,\n",
       "       -0.2111111 ,  0.671415  , -0.20082548, -0.43613642,  0.7554267 ,\n",
       "        0.13527122,  0.76433766, -0.29353505, -0.58288515,  0.55237436,\n",
       "        0.37099057,  0.42741805, -0.17804192,  0.00508157,  0.29779047,\n",
       "       -0.5246916 ,  0.17572705, -0.13934997,  0.09175885, -0.3883698 ,\n",
       "        0.00785244, -0.3638518 , -0.33835432,  0.15502752,  0.3540912 ,\n",
       "        0.3778505 , -0.1740509 ,  0.45914784, -0.12692754,  0.37269562,\n",
       "        0.8939737 , -0.3138282 ,  0.69731987,  0.19738618,  0.03562395,\n",
       "       -0.6595121 ,  0.3174727 , -0.04275811, -1.1988046 , -0.1832111 ,\n",
       "        1.0777528 ,  0.75134164,  0.5199413 , -0.9867473 ,  0.8518571 ,\n",
       "       -0.4198305 ,  0.27121592, -0.8054859 ,  0.60298884,  0.17382267,\n",
       "       -0.1905442 ,  0.10615354, -0.7999012 , -0.9017309 ,  0.29756528,\n",
       "        0.18736261, -0.75231063, -0.9322017 , -0.22858071, -1.1531627 ,\n",
       "       -0.7467491 ,  0.2761069 , -0.6811085 , -0.09565732,  0.47502452,\n",
       "        1.0285776 ,  0.00337519,  0.5565286 , -0.2871299 , -0.24659285,\n",
       "       -0.43805343,  0.18955746, -0.34992453, -0.2944582 , -0.33419913,\n",
       "       -0.22948852, -0.08728139,  0.55081   , -0.7852994 , -0.06315932,\n",
       "        0.67233205,  0.28127313,  0.45344985,  1.1677399 , -0.7269374 ,\n",
       "        0.72344005,  0.22709548,  0.40122473,  0.23611955,  0.1245311 ,\n",
       "       -0.03506491, -0.0550133 , -0.28707647, -0.3046436 ,  0.78795934,\n",
       "       -0.44668227,  0.53438365,  0.16811748,  0.06408481,  0.6084299 ,\n",
       "        0.7290818 ,  0.2480525 , -0.20868467,  0.08777395,  0.5742983 ,\n",
       "        0.937652  , -0.6432666 , -0.50258005, -0.24129629, -0.11228805,\n",
       "       -0.2162721 , -0.5795835 , -0.92591894,  0.46449932,  0.01553547,\n",
       "       -0.3551392 ,  0.6228239 ,  0.84780073, -0.2157791 , -0.40873235,\n",
       "       -0.13676667,  0.6126231 , -0.31728232, -0.14486109,  0.99056256,\n",
       "        0.08656126, -0.6848608 , -0.33879036,  0.6852823 ,  0.36497664,\n",
       "       -0.20121872, -0.61319077,  0.5226609 ,  0.873003  , -0.26071042,\n",
       "       -0.57055134,  0.57679886,  0.2852686 , -0.6014677 ,  0.23553377,\n",
       "       -0.12346478,  0.9080504 ,  0.40796936,  0.03322578, -0.09845384,\n",
       "        0.19412108, -0.62971777, -0.33065614,  0.36322677,  0.618863  ,\n",
       "        0.2552661 ,  0.72163206,  0.6474602 , -0.50229657,  0.74369836,\n",
       "       -0.4579745 , -0.18737955, -0.36908245, -0.3904105 , -0.37300128,\n",
       "        0.29708135, -0.4205113 , -0.7354946 ,  0.3946373 , -0.05932266,\n",
       "        0.14871117, -0.49588734,  0.15034351,  0.05828288,  0.0639919 ,\n",
       "        0.03295712,  0.16718851,  0.5545953 ,  0.1322551 ,  0.26607507,\n",
       "       -0.6199599 , -0.12555157, -0.40727985, -0.21949261,  0.3648607 ,\n",
       "        0.33719113,  0.9392595 ,  0.2976849 ,  0.3580591 ,  0.40460378,\n",
       "        0.33573678, -1.2112987 , -0.50755894,  0.7002096 ,  0.39500302,\n",
       "       -0.55892605, -0.23903754, -0.38954777,  0.25139374, -0.33687532,\n",
       "       -0.15905482,  0.01380772, -0.5904038 , -0.41715592,  0.309681  ,\n",
       "        0.19323531, -0.16319573, -0.37943092,  0.2704234 ,  0.6760686 ,\n",
       "       -0.14363858, -0.570616  ,  0.13150558, -0.22213224, -0.62399113,\n",
       "        0.5191511 ,  0.68302906, -0.07909809, -0.03419068,  0.8194829 ,\n",
       "       -0.6236806 ,  0.10288244,  0.07075168,  0.8119956 ,  0.23360652,\n",
       "       -0.27247176,  0.5429046 ,  0.62962055, -0.6975772 , -0.5245508 ,\n",
       "        0.38633034, -0.04362784, -0.04003346,  0.01203921, -0.22184643,\n",
       "       -0.28666595, -0.20053862,  0.53391737,  0.36565983,  0.11324272,\n",
       "        0.04641671, -0.88598704,  0.56367207, -0.2353261 ,  0.02212494,\n",
       "        0.28758168,  0.2361023 , -0.21603617,  0.48297992,  0.36166453,\n",
       "       -0.4327737 , -0.641484  ,  0.01293781,  0.39541906,  1.334408  ,\n",
       "       -0.36511883,  0.5039112 ,  0.42314786,  0.3482833 ,  0.2697279 ,\n",
       "       -0.24331398, -0.2503173 ,  0.10592914,  0.04112788, -0.31802663,\n",
       "        0.33351725,  0.09001742, -0.00360002,  0.26790726,  0.44208926,\n",
       "        0.34297198,  0.41825712, -0.9499334 , -0.05907763,  0.41914004,\n",
       "       -0.9101335 ,  0.3757182 , -0.23203442,  0.03712647,  0.05869943,\n",
       "       -0.59555143,  0.21089841,  0.23161493, -0.21497275, -0.44686776,\n",
       "       -0.02979966,  0.7010447 ,  0.14608929,  0.664606  ,  0.46332908,\n",
       "        0.03894837,  0.9384    ,  0.8091184 ,  0.15557961,  0.37690023,\n",
       "        0.75184715,  0.35336828, -0.07211348,  0.5397008 , -0.05698923,\n",
       "       -0.5553915 ,  0.8627012 ,  0.36897263, -0.22042134,  0.43635118,\n",
       "       -0.7712375 ,  0.2037957 ,  0.26132423, -0.04962991, -0.4148354 ,\n",
       "       -0.14738195, -0.89169186,  0.8769789 , -0.09294081, -0.25565535,\n",
       "        0.40687764, -0.05513836,  0.06000344,  0.7525879 , -0.7713722 ,\n",
       "        0.39902213,  0.426192  , -1.1785604 ,  0.21124244,  0.54497474,\n",
       "       -0.7676891 ,  0.26705796,  0.13122039,  0.42630097, -1.0652066 ,\n",
       "        0.37210596,  0.20436053,  0.975175  , -0.4907471 ,  0.5326514 ,\n",
       "        0.7240444 ,  0.9420783 ,  0.6537005 ,  0.01164002, -0.49563175,\n",
       "       -0.07218332, -0.14715092, -0.810258  , -0.16917321,  0.24460232,\n",
       "        0.57908446, -0.6916574 ,  0.04842933, -0.46591187,  0.34683272,\n",
       "        0.1651842 , -0.8561251 , -0.7962817 ,  0.23766063, -0.17231876,\n",
       "        0.18722796, -0.30731046, -0.7310927 , -0.32235676, -0.12291042,\n",
       "        0.6171415 ,  0.9958565 ,  0.08993081,  0.41989493,  0.74975896,\n",
       "       -0.54169196,  0.0379312 , -0.05499844, -0.4391762 ,  0.46438122,\n",
       "       -0.62578255, -0.0034908 , -0.71085536, -0.8445743 ,  0.35079336,\n",
       "       -0.27118325, -1.0940752 , -1.0629492 , -0.93851817, -0.43656936,\n",
       "       -0.20247369, -0.12330224, -0.53172845,  1.039315  , -0.3422052 ,\n",
       "       -0.3983778 , -0.11943599, -0.34787646, -0.23199283,  0.6243685 ,\n",
       "        0.03491597, -0.5644536 ,  0.3606066 ,  0.1704313 ,  0.02930531,\n",
       "        0.72914445, -0.24580926, -0.00743662,  0.17504513,  0.2599282 ,\n",
       "       -0.20504434,  0.15938155, -0.20723313, -0.8686638 ,  0.12319616,\n",
       "       -1.1089745 , -0.7262378 ,  0.96023965,  0.21190207,  0.05931135,\n",
       "        0.25081354, -0.23690338,  0.20616022, -0.0152061 , -0.50526744,\n",
       "       -0.01944478, -0.2609934 ,  0.1361605 ,  0.5170224 , -0.38378316,\n",
       "        0.9156151 ,  0.5600166 , -0.1654605 ,  0.24690992, -0.34363556,\n",
       "       -0.10887881, -0.8181026 ,  0.4852836 , -0.3897916 ,  0.05515533,\n",
       "       -0.06947099, -0.16562015, -1.1104302 , -0.67495894,  0.0558234 ,\n",
       "       -0.02117754,  0.06035815, -0.29744586, -0.38275075,  0.31124318,\n",
       "       -0.09818288, -0.34012318, -0.33294368, -0.9772637 , -0.02797174,\n",
       "        0.17772026, -0.27024046,  0.39010936, -0.54897654, -0.1881468 ,\n",
       "        0.18207781,  0.5091239 , -0.6501176 , -0.09800472, -0.5589907 ,\n",
       "       -1.2525911 , -0.5630804 ,  0.04313761,  0.07995319,  0.58251727,\n",
       "       -0.07235278, -0.21591428, -0.5256531 , -0.4701674 , -0.7012625 ,\n",
       "        0.08048212, -0.19965577, -0.3462831 , -0.8331401 ,  0.14752398,\n",
       "       -0.8124902 , -0.34464163,  0.3895827 ,  0.75337505, -0.16767134,\n",
       "       -0.23763685,  0.04914769,  0.15028594, -0.07816049, -0.09053901,\n",
       "       -0.9944961 , -1.4353917 ,  0.17090729,  0.23493657,  0.13495806,\n",
       "       -0.89312917,  0.2705213 ,  0.3863235 , -0.08920502, -0.46040642,\n",
       "        0.53904486, -0.497172  ,  0.8259955 ,  0.7572632 , -0.03119388,\n",
       "        0.01291046,  0.92947483, -0.4787556 , -0.33923963,  0.3902513 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_attention(generated_text:List[str], context_window:List[str], embed):\n",
    "    \n",
    "    memory = compute_avg_weight_from_string(generated_text,embed)\n",
    "    context = compute_avg_weight_from_string(context_window,embed)\n",
    "    \n",
    "    attention = compute_avg_weight([memory,context])\n",
    "    \n",
    "    return attention\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "generated_text = [\"i\", \"love\",\"japan\"]  \n",
    "context_window = [\"love\",\"japan\"]\n",
    "\n",
    "att = get_attention(generated_text,context_window, embed)\n",
    "att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2ff6f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 0.8199621438980103),\n",
       " ('crazy', 0.6153724789619446),\n",
       " ('me', 0.609982967376709),\n",
       " ('you', 0.592400848865509),\n",
       " ('japan', 0.5758095383644104),\n",
       " ('europe', 0.5509176850318909),\n",
       " ('madonna', 0.5494800209999084),\n",
       " ('everything', 0.5483258366584778),\n",
       " ('ambition', 0.5454602241516113),\n",
       " ('beyoncé', 0.5414654016494751)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words = embed.wv.most_similar(positive=[att], topn=10)\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b167f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_choice(embed, memory, context, values):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16761201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe60f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402b1082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import os\n",
    "from torchvision.datasets.utils import download_url\n",
    "from typing import Tuple\n",
    "import sys\n",
    "import time \n",
    "import random \n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "class TripleGramLLM:\n",
    "    def __init__(self,corpus:str):\n",
    "        self.corpus = corpus\n",
    "        self.model = {}\n",
    "        self.tokenized = self.corpus.lower().split()        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.model)\n",
    "        \n",
    "    def train(self):\n",
    "        model = self.model\n",
    "                \n",
    "        ngram_triplets = ngrams(self.tokenized, 3)\n",
    "\n",
    "        for word1, word2, word3 in ngram_triplets:\n",
    "            key = (word1, word2)\n",
    "            if key in model:\n",
    "                model[key].append(word3)\n",
    "            else:\n",
    "                model[key] = [word3]\n",
    "\n",
    "        return model\n",
    " \n",
    "    def _generate_from_seed(self, seed : Tuple[str,str], length:int = 20):\n",
    "#         seed = [e.lower() for e in seed]\n",
    "        model = self.model\n",
    "        current_word = seed[1]\n",
    "        prev_word = seed[0]\n",
    "        generated_text = [prev_word, current_word]\n",
    "\n",
    "        for _ in range(length - 1):\n",
    "            key = (prev_word, current_word)\n",
    "            if key in model:\n",
    "                next_word = random.choice(model[key])\n",
    "                generated_text.append(next_word)\n",
    "                prev_word = current_word\n",
    "                current_word = next_word\n",
    "            else:\n",
    "                next_word = random.choice(self.tokenized)\n",
    "                generated_text.append(next_word)\n",
    "                prev_word = current_word\n",
    "                current_word = next_word\n",
    "        return generated_text\n",
    "                \n",
    "    def generate_from_seed(self, seed : Tuple[str,str], length:int = 20, streaming=False):\n",
    "        text = ' '.join(self._generate_from_seed(seed=seed, length=length))\n",
    "        \n",
    "        if streaming:\n",
    "            for char in text:\n",
    "\n",
    "                sys.stdout.write(char)\n",
    "                sys.stdout.flush()\n",
    "                time.sleep(0.02)  # Optional delay for demonstration purposes\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def save_model(self):\n",
    "        with open('triple_gram_model.pkl','wb') as f:\n",
    "            pickle.dump(llm, f)\n",
    "            \n",
    "    @staticmethod\n",
    "    def load_model():\n",
    "        with open('triple_gram_model.pkl','rb') as f:\n",
    "            return pickle.load(f) \n",
    " \n",
    "\n",
    "min_corpus = raw_corpus[:3000000]\n",
    "llm = TripleGramLLM(min_corpus)\n",
    "llm.train()\n",
    "len(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2f500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = llm.generate_from_seed(('japan','is'),length=100)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5148a71c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f944cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1360f65f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ddefb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4617602f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd9bbc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "01b8ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##large_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5e21d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15da4db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6459fc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b94cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd750b74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
