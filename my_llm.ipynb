{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a8efcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\khale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a7a243",
   "metadata": {},
   "source": [
    "# WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "037bfb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence 1: ['I', 'love', 'programming', '.']\n",
      "Tokenized sentence 2: ['NLTK', 'is', 'a', 'great', 'toolkit', 'for', 'natural', 'language', 'processing', '.']\n",
      "Tokenized sentence 3: ['Tokenization', 'is', 'an', 'important', 'step', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentences = [\n",
    "    \"I love programming.\",\n",
    "    \"NLTK is a great toolkit for natural language processing.\",\n",
    "    \"Tokenization is an important step in NLP.\"\n",
    "]\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Print the tokenized sentences\n",
    "for i, sentence in enumerate(tokenized_sentences, 1):\n",
    "    print(f\"Tokenized sentence {i}: {sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99299af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\software\\data-science-explorer\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)/main/tokenizer.json: 100%|█████████████████████████████████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 810kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs for sentence 1: [101, 1045, 2293, 4730, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Token IDs for sentence 2: [101, 17953, 2102, 2243, 2003, 1037, 2307, 6994, 23615, 2005, 3019, 2653, 6364, 1012, 102]\n",
      "Token IDs for sentence 3: [101, 19204, 3989, 2003, 2019, 2590, 3357, 1999, 17953, 2361, 1012, 102, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "sentences = [\n",
    "    \"I love programming.\",\n",
    "    \"NLTK is a great toolkit for natural language processing.\",\n",
    "    \"Tokenization is an important step in NLP.\"\n",
    "]\n",
    "\n",
    "# Load a pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the sentences and obtain token IDs\n",
    "tokenized_inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get the token IDs as a list\n",
    "token_ids = tokenized_inputs[\"input_ids\"].tolist()\n",
    "\n",
    "# Print the token IDs\n",
    "for i, ids in enumerate(token_ids, 1):\n",
    "    print(f\"Token IDs for sentence {i}: {ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01f65f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] i love programming. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] nltk is a great toolkit for natural language processing. [SEP]\n",
      "[CLS] tokenization is an important step in nlp. [SEP] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# Print the decoded tokens\n",
    "for i, ids in enumerate(token_ids):\n",
    "    # Convert token IDs to string\n",
    "    decoded_tokens = tokenizer.decode(ids)\n",
    "    print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eef64b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.325103  , -1.0814596 , -0.19151801, -1.0917969 ,  0.3774717 ,\n",
       "        2.1193674 ,  0.14831299,  1.1750474 ,  0.27608478,  3.5640159 ,\n",
       "       -0.21550748, -0.3872805 , -0.01346767, -1.378101  ,  1.5121084 ,\n",
       "       -0.08623586, -1.933989  , -0.11408056, -0.9860206 , -2.4706352 ,\n",
       "       -1.1276447 , -0.4152274 ,  2.1197853 ,  1.7633233 , -0.2526904 ,\n",
       "       -0.7457644 ,  1.0953835 ,  0.11857101,  0.61220956, -1.3049716 ,\n",
       "        1.3378174 , -0.29886734, -0.29212442, -0.4469438 ,  0.35449713,\n",
       "       -2.0813339 , -2.4365878 , -1.4595014 , -0.13775744, -2.192705  ,\n",
       "        1.5707886 , -0.27742532,  0.73836327,  1.3525312 , -0.7743389 ,\n",
       "       -0.77003706, -0.02322291,  0.77104056,  0.31869486, -0.13776901,\n",
       "        0.19484688, -1.2495929 ,  0.498856  , -0.00436315, -0.16236602,\n",
       "       -0.6067438 ,  0.8008451 ,  2.8110933 ,  0.69573265, -0.7279365 ,\n",
       "       -0.3188081 , -1.1669003 ,  1.6769878 , -1.578182  , -0.14196508,\n",
       "       -1.8940164 ,  1.4671096 ,  1.7788589 , -0.8679901 , -0.15584452,\n",
       "       -0.13961807, -0.09730375,  0.49901256, -0.45330203,  0.8772218 ,\n",
       "       -1.2259632 , -0.03491092,  2.9636195 , -1.7653301 , -0.90529907,\n",
       "        0.5244117 ,  0.16144626, -2.2888186 , -0.6361609 ,  0.7979543 ,\n",
       "       -0.9861773 ,  1.9256904 ,  0.7562646 ,  0.38272035,  2.296901  ,\n",
       "        0.37406802,  0.3630618 ,  0.5928567 , -0.8194863 ,  0.130174  ,\n",
       "        1.5654953 ,  0.9809555 ,  0.63848054,  1.4237264 , -1.2110319 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.wv['japan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "078afaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Words to 'country': [('region', 0.7159584164619446), ('land', 0.6805279850959778), ('nation', 0.658952534198761), ('sector', 0.6567452549934387), ('measure', 0.6284477114677429), ('village', 0.6263555884361267), ('garden', 0.6238498091697693), ('town', 0.6204319596290588), ('wilderness', 0.6184377074241638), ('economy', 0.6140435338020325)]\n"
     ]
    }
   ],
   "source": [
    "# Get word vector for a specific word\n",
    "def compare_words(word, model):\n",
    "    word_vector = model.wv[word]\n",
    "\n",
    "    # Find similar words\n",
    "    similar_words = model.wv.most_similar(word)\n",
    "\n",
    "    # Print results\n",
    "#     print(f\"Word Vector for '{word}': {word_vector}\")\n",
    "    print(f\"Similar Words to '{word}': {similar_words}\")\n",
    "    \n",
    "    return similar_words\n",
    "    \n",
    "word = \"country\"\n",
    "_ = compare_words(word, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c85caac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Key 'lately?' not present\"\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the state of Japan lately?\"\n",
    "\n",
    "new_sentence = query\n",
    "# Split the sentence into words\n",
    "words = new_sentence.lower().split()\n",
    "\n",
    "oov_vector = [0.0] * model.vector_size  # Assign a random vector or all zeros\n",
    "\n",
    "\n",
    "# Compute the average embedding\n",
    "sentence_embedding_sum = []\n",
    "\n",
    "for word in words:\n",
    "    try:\n",
    "        vec = model.wv[word]\n",
    "    except KeyError as error:\n",
    "        print(error)\n",
    "        vec = oov_vector\n",
    "    sentence_embedding_sum.append(vec)\n",
    "        \n",
    "    \n",
    "    \n",
    "sentence_embedding = sum(sentence_embedding_sum) / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9cbbb9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.6684510707855225\n",
      "concern 0.6658481359481812\n",
      "policy 0.6452551484107971\n",
      "measure 0.6338269114494324\n",
      "view 0.6305679678916931\n",
      "country 0.6284566521644592\n",
      "world 0.6249071955680847\n",
      "situation 0.6224737167358398\n",
      "problems 0.620403528213501\n",
      "economy 0.6164305806159973\n"
     ]
    }
   ],
   "source": [
    "# Find the most similar words\n",
    "similar_words = model.wv.most_similar(positive=[sentence_embedding], topn=10)\n",
    "\n",
    "# Print the most similar words\n",
    "for word, similarity in similar_words:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97f407e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Vector for 'I': [-4.88621801e-01 -1.40371811e+00 -1.39933318e-01  1.06850706e-01\n",
      "  1.02041435e+00 -2.22615689e-01  2.75205951e-02  1.12041712e+00\n",
      " -1.21569169e+00  1.12856403e-01 -8.95855650e-02  8.30142871e-02\n",
      " -6.56571686e-01  1.30964851e+00  3.64047140e-01  5.38011491e-01\n",
      " -5.37430048e-01 -1.51941374e-01  4.55765158e-01  3.98258448e-01\n",
      " -1.38316467e-01 -4.49481159e-01 -1.51659131e+00 -1.16060197e+00\n",
      "  4.38162953e-01  1.36776134e-01 -8.98952782e-01  3.42559814e-01\n",
      " -2.75909845e-02 -6.08607709e-01 -1.12204149e-01  4.31124866e-01\n",
      "  4.22272563e-01  3.68530124e-01 -2.09876690e-02 -1.09747119e-01\n",
      " -1.22736312e-01  1.95901453e-01 -2.79443003e-02  8.52847695e-01\n",
      " -2.90774971e-01 -3.17778558e-01  9.51572239e-01  1.95836142e-01\n",
      " -1.57318980e-01 -5.18942475e-01  4.68272626e-01 -5.29799104e-01\n",
      "  4.45801914e-01 -3.91309671e-02  3.04106951e-01 -3.67694885e-01\n",
      "  2.44503692e-01  6.94566607e-01 -2.64469862e-01  2.73315817e-01\n",
      "  3.21534514e-01 -1.09112464e-01  3.19945246e-01 -5.23173511e-01\n",
      "  1.46419764e-01 -1.48782870e-02  7.48982787e-01  2.35898018e-01\n",
      " -1.78792453e+00  1.02411890e+00  8.47361386e-01 -2.40542763e-03\n",
      "  7.93799520e-01  1.68916142e+00 -4.51332897e-01 -2.24225700e-01\n",
      "  5.35463020e-02 -1.44621148e-03  7.98828602e-01  7.86827445e-01\n",
      "  4.34700429e-01  1.54868081e-01 -3.42725873e-01 -1.92696422e-01\n",
      " -1.90006882e-01 -1.07769752e-02 -8.26064289e-01  3.10222358e-01\n",
      " -2.57125050e-01  5.56767583e-01 -8.59422922e-01 -4.07058448e-01\n",
      " -1.70669138e-01 -9.14901257e-01  6.96151793e-01  8.87783229e-01\n",
      " -1.30843788e-01  4.37764913e-01 -2.14055017e-01  8.04235280e-01\n",
      "  9.14338648e-01 -6.14193499e-01 -3.08949679e-01  2.72023022e-01]\n",
      "Similar Words to 'I': [('Y', 0.4218229055404663), ('y', 0.252797394990921), ('B', 0.21075616776943207), (']', 0.2087993025779724), ('\"', 0.18925084173679352), ('/', 0.17815521359443665), ('W', 0.16929380595684052), ('D', 0.16154971718788147), ('*', 0.14543616771697998), (\"'\", 0.12986239790916443)]\n"
     ]
    }
   ],
   "source": [
    "compare_words(word, model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd3e1ad",
   "metadata": {},
   "source": [
    "# NGRAM models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbe73e2",
   "metadata": {},
   "source": [
    "## 2gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e5812963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "Hello\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from nltk import ngrams\n",
    "\n",
    "def generate_2gram_model(text):\n",
    "    model = {}\n",
    "    ngram_pairs = ngrams(text.split(), 2)\n",
    "    \n",
    "    for word1, word2 in ngram_pairs:\n",
    "        if word1 in model:\n",
    "            model[word1].append(word2)\n",
    "        else:\n",
    "            model[word1] = [word2]\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_text(model, seed, length):\n",
    "    current_word = seed\n",
    "    generated_text = [current_word]\n",
    "    \n",
    "    for _ in range(length - 1):\n",
    "        if current_word in model:\n",
    "            next_word = random.choice(model[current_word])\n",
    "            generated_text.append(next_word)\n",
    "            current_word = next_word\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return ' '.join(generated_text)\n",
    "\n",
    "corpus = \" <end> \".join(sentences)\n",
    "print(len(corpus))\n",
    "# Example usage\n",
    "\n",
    "text = corpus\n",
    "model = generate_2gram_model(text)\n",
    "generated_text = generate_text(model, seed=\"Hello\", length=10)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "05f3ec98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(model, seed=\"Hello\", length=10)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbd72fe",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6694f13",
   "metadata": {},
   "source": [
    "## gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb2858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab97b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ed18b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "65d550d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70f38aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def get_sample(corpus_object):\n",
    "    file_ids = corpus_object.fileids()\n",
    "    return [corpus_object.sents(file_id) for file_id in file_ids]\n",
    "\n",
    "def transform_into_sentences(corpus_object):\n",
    "    sentences = []\n",
    "    file_ids = corpus_object.fileids()\n",
    "\n",
    "    # Extract sentences from each file in the Gutenberg Corpus\n",
    "    for file_id in file_ids:\n",
    "        file_sentences = corpus_object.sents(file_id)\n",
    "        for sentence in file_sentences:\n",
    "            # Convert the sentence list to a string\n",
    "            sentence_str = ' '.join(sentence)\n",
    "            sentences.append(sentence_str)\n",
    "    return sentences\n",
    "    \n",
    "def get_data_from_nltk():\n",
    "    corpus_list = [\n",
    "        'pros_cons','reuters','sentiwordnet','gutenberg','senseval',\n",
    "        'sinica_treebank','comparative_sentences',\n",
    "        'twitter'\n",
    "    ]\n",
    "\n",
    "    for item in corpus_list:\n",
    "        try:\n",
    "            nltk.download(item)\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            \n",
    "    from nltk.corpus import pros_cons\n",
    "    from nltk.corpus import sentiwordnet, pros_cons,reuters,sentiwordnet,gutenberg,senseval, sinica_treebank,comparative_sentences\n",
    "    from nltk.corpus import TwitterCorpusReader\n",
    "    corpus_objects_ls = [ pros_cons,reuters ,gutenberg, sinica_treebank,comparative_sentences\n",
    "    ]\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "                \n",
    "    nltk_total_corpus = []\n",
    "    for c in corpus_objects_ls:\n",
    "        nltk_total_corpus.extend(transform_into_sentences(c))\n",
    "        \n",
    "    print(nltk_total_corpus[100:105])\n",
    "    \n",
    "    return nltk_total_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3840ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import pros_cons\n",
    "from nltk.corpus import sentiwordnet, pros_cons,reuters,sentiwordnet,gutenberg,senseval, sinica_treebank,comparative_sentences\n",
    "from nltk.corpus import TwitterCorpusReader\n",
    "corpus_objects_ls = [ pros_cons,reuters ,gutenberg, sinica_treebank,comparative_sentences\n",
    "]\n",
    "\n",
    "\n",
    "def get_sample(corpus_object):\n",
    "    file_ids = corpus_object.fileids()\n",
    "    return [corpus_object.sents(file_id) for file_id in file_ids]\n",
    "\n",
    "def transform_into_sentences(corpus_object):\n",
    "    sentences = []\n",
    "    file_ids = corpus_object.fileids()\n",
    "\n",
    "    # Extract sentences from each file in the Gutenberg Corpus\n",
    "    for file_id in file_ids:\n",
    "        file_sentences = corpus_object.sents(file_id)\n",
    "        for sentence in file_sentences:\n",
    "            # Convert the sentence list to a string\n",
    "            sentence_str = ' '.join(sentence)\n",
    "            sentences.append(sentence_str)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "            \n",
    "nltk_total_corpus = []\n",
    "for c in corpus_objects_ls:\n",
    "    nltk_total_corpus.extend(transform_into_sentences(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98e730fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package pros_cons to\n",
      "[nltk_data]     C:\\Users\\khale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package pros_cons is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\khale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\khale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\khale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package senseval to\n",
      "[nltk_data]     C:\\Users\\khale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package senseval is already up-to-date!\n",
      "[nltk_data] Downloading package sinica_treebank to\n",
      "[nltk_data]     C:\\Users\\khale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data] Downloading package comparative_sentences to\n",
      "[nltk_data]     C:\\Users\\khale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package comparative_sentences is already up-to-date!\n",
      "[nltk_data] Error loading twitter: Package 'twitter' not found in\n",
      "[nltk_data]     index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['printing photos is hard for me', 'eats batteries quickly , viewfinder too far from lens , sporadic LCD window', \"As with all digital cam ' s ... eats batteries .\", 'Batteries can go fast if you dont get good ones', 'Has flaws which are made worse by bad service policy .']\n"
     ]
    }
   ],
   "source": [
    "            \n",
    "nltk_total_corpus = get_data_from_nltk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afbb1f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['printing photos is hard for me',\n",
       " 'eats batteries quickly , viewfinder too far from lens , sporadic LCD window',\n",
       " \"As with all digital cam ' s ... eats batteries .\",\n",
       " 'Batteries can go fast if you dont get good ones',\n",
       " 'Has flaws which are made worse by bad service policy .']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_total_corpus[100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2982fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b0eb84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5dcae26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 36718\n",
      "1 1801350\n",
      "2 error!\n",
      "2 12500\n",
      "3 error!\n",
      "3 120000\n",
      "4 error!\n",
      "4 560000\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import WikiText2, WikiText103, YelpReviewPolarity, AG_NEWS, IMDB\n",
    "\n",
    "datasets = [WikiText2, WikiText103, IMDB, AG_NEWS, YelpReviewPolarity]\n",
    "\n",
    "# Load train, val, and test splits from each dataset\n",
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "for index, dataset in enumerate(datasets):\n",
    "    try:\n",
    "        train_dataset, val_dataset, test_dataset = dataset()\n",
    "        train_dataset = list(train_dataset)\n",
    "        print(index, len(train_dataset))\n",
    "        train_data.extend(train_dataset)\n",
    "        val_data.extend(val_dataset)\n",
    "        test_data.extend(test_dataset)\n",
    "    except:\n",
    "        print(index, 'error!')\n",
    "        train_dataset,  test_dataset = dataset()\n",
    "        train_dataset,  test_dataset = list(train_dataset),  list(test_dataset)\n",
    "        print(index, len(train_dataset))\n",
    "        train_data.extend([e for e in train_dataset])\n",
    "        test_data.extend([e for e in test_dataset])\n",
    "\n",
    "combined_data = train_data + val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d0b0cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_token=\"<end>\\n\" \n",
    "def get_corpus(docs, end_token=\"<end>\\n\"):\n",
    "    return end_token.join(str(d).lower() for d in docs)\n",
    "\n",
    "large_corpus = get_corpus(combined_data + nltk_total_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf52f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json \n",
    "\n",
    "with open('large_corpus_nltk_pytorch.json','w') as f:\n",
    "    json.dump({\"data\":large_corpus},f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9618f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle  \n",
    "    \n",
    "with open('large_corpus_nltk_pytorch.json','rb') as f:\n",
    "    raw_corpus = json.load(f)['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71ecba98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050257143"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f43441c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2755229"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = raw_corpus.lower().split(\"<end>\\n\")\n",
    "len(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0dcb9a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the game \\'s battle system , the <unk> system , is carried over directly from <unk> chronicles . during missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . a character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \\' turns . each character has a field and distance of movement limited by their action <unk> . up to nine characters can be assigned to a single mission . during gameplay , characters will call out if something happens to them , such as their health points ( hp ) getting low or being knocked out by enemy attacks . each character has specific \" potentials \" , skills unique to each character . they are divided into \" personal potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" battle potentials \" , which are grown throughout the game and always grant <unk> to a character . to learn battle potentials , each character has a unique \" masters table \" , a grid @-@ based skill table that can be used to acquire and link different skills . characters also have special <unk> that grant them temporary <unk> on the battlefield : kurt can activate \" direct command \" and move around the battlefield without <unk> his action point gauge , the character <unk> can shift into her \" valkyria form \" and become <unk> , while imca can target multiple enemy units with her heavy weapon . \\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52c74680",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_vectorize = [e.split() for e in tokenized if e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "341173d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], ['=', 'valkyria', 'chronicles', 'iii', '=']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_vectorize[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d15ff90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "embed = Word2Vec(sentences=to_vectorize, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb0d99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5abc059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_embedding(word, embed, verbose=False):\n",
    "    try:\n",
    "        return embed.wv[word]\n",
    "    except KeyError as error:\n",
    "        if verbose:\n",
    "            print(error)\n",
    "        return np.array([0.0] * embed.vector_size,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bc59028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Words to 'demon': [('demonic', 0.787643313407898), ('sorceress', 0.7807117700576782), ('hades', 0.7756515741348267), ('demons', 0.7661198377609253), ('vader', 0.7545596361160278), ('minions', 0.7517493367195129), ('raziel', 0.751736581325531), ('robot', 0.7492826581001282), ('malevolent', 0.7461868524551392), ('reaper', 0.7458856105804443)]\n"
     ]
    }
   ],
   "source": [
    "# Get word vector for a specific word\n",
    "def compare_words(word, model):\n",
    "    word_vector = get_embedding(word,model)\n",
    "\n",
    "    # Find similar words\n",
    "    similar_words = model.wv.most_similar(word)\n",
    "\n",
    "    # Print results\n",
    "#     print(f\"Word Vector for '{word}': {word_vector}\")\n",
    "    print(f\"Similar Words to '{word}': {similar_words}\")\n",
    "    \n",
    "    return similar_words\n",
    "\n",
    "cmp = compare_words('demon', embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55b9b058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.05919611e-01,  1.14142525e+00,  3.60883623e-01, -7.34292448e-01,\n",
       "        1.74941897e-01,  3.58267516e-01, -1.76467574e+00, -1.61167488e-01,\n",
       "        4.84647512e-01,  6.78894341e-01, -4.56032842e-01,  1.41198948e-01,\n",
       "       -1.40476811e+00, -1.24829936e+00,  1.45018566e+00,  1.87893316e-01,\n",
       "       -8.27334404e-01, -1.11700080e-01,  6.97735727e-01, -7.26623595e-01,\n",
       "        1.70982277e+00, -2.32270315e-01,  4.17005301e-01, -6.24032199e-01,\n",
       "        4.16980833e-01, -5.33148833e-02,  7.14183390e-01,  3.19495648e-01,\n",
       "       -1.16440214e-01,  6.27054989e-01, -9.53394473e-02,  2.95685917e-01,\n",
       "       -1.10113561e+00,  2.00224385e-01, -6.54533356e-02,  8.01723659e-01,\n",
       "        5.48441112e-01, -1.09596241e+00,  2.30886444e-01, -1.90004215e-01,\n",
       "       -7.10231245e-01, -2.27177992e-01,  7.68104494e-01, -7.67107546e-01,\n",
       "        1.76577285e-01,  1.25132048e+00,  1.75284028e-01, -6.10024035e-01,\n",
       "        1.84524596e-01, -1.55849588e+00,  1.63258743e+00,  1.47438002e+00,\n",
       "        1.50979900e+00, -9.55390990e-01, -5.69579959e-01, -3.51171523e-01,\n",
       "        5.82250237e-01,  7.41088688e-01, -5.96331894e-01,  8.62585306e-01,\n",
       "       -3.41105610e-01,  9.19178724e-01, -7.41110086e-01,  1.46654856e+00,\n",
       "        1.06800020e+00,  3.87049049e-01,  1.28747070e+00, -9.47720781e-02,\n",
       "        5.97063243e-01,  1.88509655e+00, -5.27722895e-01,  1.47372019e+00,\n",
       "        6.90077782e-01,  4.50112335e-02, -1.06206548e+00, -3.94687146e-01,\n",
       "        8.98554385e-01, -6.00840032e-01, -7.81824589e-01, -3.26024860e-01,\n",
       "       -3.51568103e-01,  3.99214357e-01, -1.32373020e-01, -5.86246312e-01,\n",
       "        2.38126233e-01, -6.53988957e-01,  2.87823886e-01,  5.83156109e-01,\n",
       "       -2.74867695e-02,  3.81917387e-01, -2.07871723e+00,  1.58663478e-03,\n",
       "       -7.06658363e-01,  1.14447773e+00, -2.75349641e+00, -1.06793487e+00,\n",
       "        5.30278385e-01,  1.04194081e+00,  2.77719229e-01,  1.39380217e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_avg_weight(query:str, embed):\n",
    "    query_words = query.split()\n",
    "    return sum(get_embedding(word, embed) for word in query_words) / len(query_words)\n",
    "compute_avg_weight(\"I love Japan\", embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7bc6a060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24941416"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fad5b4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050257143"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a6504b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_context(generated_text:List[str], ngram_window_tokens:List[str]):\n",
    "    \n",
    "    generated_text_vecs = [compute_avg_weight(v, embed) for v in tokens]\n",
    "    ngram_window_vecs = [compute_avg_weight(v, embed) for v in ngram_window_tokens]\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b167f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16761201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe60f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "402b1082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218558"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle \n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import os\n",
    "from torchvision.datasets.utils import download_url\n",
    "from typing import Tuple\n",
    "import sys\n",
    "import time \n",
    "import random \n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "class TripleGramLLM:\n",
    "    def __init__(self,corpus:str):\n",
    "        self.corpus = corpus\n",
    "        self.model = {}\n",
    "        self.tokenized = self.corpus.lower().split()        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.model)\n",
    "        \n",
    "    def train(self):\n",
    "        model = self.model\n",
    "                \n",
    "        ngram_triplets = ngrams(self.tokenized, 3)\n",
    "\n",
    "        for word1, word2, word3 in ngram_triplets:\n",
    "            key = (word1, word2)\n",
    "            if key in model:\n",
    "                model[key].append(word3)\n",
    "            else:\n",
    "                model[key] = [word3]\n",
    "\n",
    "        return model\n",
    " \n",
    "    def _generate_from_seed(self, seed : Tuple[str,str], length:int = 20):\n",
    "#         seed = [e.lower() for e in seed]\n",
    "        model = self.model\n",
    "        current_word = seed[1]\n",
    "        prev_word = seed[0]\n",
    "        generated_text = [prev_word, current_word]\n",
    "\n",
    "        for _ in range(length - 1):\n",
    "            key = (prev_word, current_word)\n",
    "            if key in model:\n",
    "                next_word = random.choice(model[key])\n",
    "                generated_text.append(next_word)\n",
    "                prev_word = current_word\n",
    "                current_word = next_word\n",
    "            else:\n",
    "                next_word = random.choice(self.tokenized)\n",
    "                generated_text.append(next_word)\n",
    "                prev_word = current_word\n",
    "                current_word = next_word\n",
    "        return generated_text\n",
    "                \n",
    "    def generate_from_seed(self, seed : Tuple[str,str], length:int = 20, streaming=False):\n",
    "        text = ' '.join(self._generate_from_seed(seed=seed, length=length))\n",
    "        \n",
    "        if streaming:\n",
    "            for char in text:\n",
    "\n",
    "                sys.stdout.write(char)\n",
    "                sys.stdout.flush()\n",
    "                time.sleep(0.02)  # Optional delay for demonstration purposes\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def save_model(self):\n",
    "        with open('triple_gram_model.pkl','wb') as f:\n",
    "            pickle.dump(llm, f)\n",
    "            \n",
    "    @staticmethod\n",
    "    def load_model():\n",
    "        with open('triple_gram_model.pkl','rb') as f:\n",
    "            return pickle.load(f) \n",
    " \n",
    "\n",
    "min_corpus = raw_corpus[:3000000]\n",
    "llm = TripleGramLLM(min_corpus)\n",
    "llm.train()\n",
    "len(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2f2f500b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'japan is virtue force important their , difficult . <unk> military tactics and goals = = reception = = <end> <end> = = <end> <end> = = track listing = = = biography = = = = <end> <end> = = <end> <end> the song is written from constantine \\'s history appear , later killed , were more common relative , and flowers . barker was unable to . i , \" space walk \" and another album track called \" dirt \" experience . the mint of christopher <unk> . <end> ballcourt 2 is the night was a lot of'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = llm.generate_from_seed(('japan','is'),length=100)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5148a71c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f944cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1360f65f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ddefb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4617602f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd9bbc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "01b8ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##large_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5e21d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15da4db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6459fc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b94cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd750b74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
